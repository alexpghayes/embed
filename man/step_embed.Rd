% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glm.R
\name{step_embed}
\alias{step_embed}
\alias{tidy.step_embed}
\title{Encoding Factors into Linear Functions}
\usage{
step_embed(recipe, ..., role = NA, trained = FALSE, outcome = NULL,
  pooling = FALSE, stan = TRUE, options = list(seed =
  sample.int(10^5, 1)), verbose = FALSE, mapping = NULL,
  skip = FALSE)

\method{tidy}{step_embed}(x, ...)
}
\arguments{
\item{recipe}{A recipe object. The step will be added to the
sequence of operations for this recipe.}

\item{...}{One or more selector functions to choose variables.
For \code{step_embed}, this indicates the variables to be encoded
into a numeric format. See \code{\link[recipes:selections]{recipes::selections()}} for more details. For
the \code{tidy} method, these are not currently used.}

\item{role}{Not used by this step since no new variables are
created.}

\item{trained}{A logical to indicate if the quantities for
preprocessing have been estimated.}

\item{outcome}{A call to \code{vars} to specify which variable is
used as the outcome in the generalized linear model. Only
numeric and two-level factors are currently supported.}

\item{pooling}{A logical; should partial pooling be used via a hierarchical
generalized linear model?}

\item{stan}{A logical indicating if Stan should be used to estimate the
embedding. Defaults to \code{TRUE}. When \code{FALSE}, the embedding is estimated
with \code{lme4} instead. In general, \code{lme4} should be much faster, but does
allow for as much control over the amount of partial pooling. Ignored when
\code{pooling = FALSE}.}

\item{options}{A list of options to pass to functions that estimate
partially pooled embeddings. When \code{stan = TRUE} (the default), passed to
\code{\link[rstanarm:stan_glmer]{rstanarm::stan_glmer()}}. When \code{stan = FALSE}, passed to \code{\link[lme4:lmer]{lme4::lmer()}}
for regression problems and \code{\link[lme4:glmer]{lme4::glmer()}} for classification problems.}

\item{verbose}{A logical to control the default printing by
\code{\link[rstanarm:stan_glmer]{rstanarm::stan_glmer()}}. Ignored when \code{stan = FALSE}.}

\item{mapping}{A list of tibble results that define the
encoding. This is \code{NULL} until the step is trained by
\code{\link[recipes:prep.recipe]{recipes::prep.recipe()}}.}

\item{skip}{A logical. Should the step be skipped when the
recipe is baked by \code{\link[recipes:bake.recipe]{recipes::bake.recipe()}}? While all operations are baked
when \code{\link[recipes:prep.recipe]{recipes::prep.recipe()}} is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using \code{skip = TRUE} as it may affect
the computations for subsequent operations}

\item{x}{A \code{step_embed} object.}
}
\value{
An updated version of \code{recipe} with the new step added
to the sequence of existing steps (if any). For the \code{tidy}
method, a tibble with columns \code{terms} (the selectors or
variables for encoding), \code{level} (the factor levels), and
\code{value} (the encodings).
}
\description{
\code{step_embed} creates a \emph{specification} of a recipe step that
will convert a nominal (i.e. factor) predictor into a single set of
scores derived from a generalized linear model.
}
\details{
For each factor predictor, a generalized linear model
is fit to the outcome and the coefficients are returned as the
encoding. These coefficients are on the linear predictor scale
so, for factor outcomes, they are in log-odds units. The
coefficients are created using a no intercept model and, when
two factor outcomes are used, the log-odds reflect the event of
interest being the \emph{first} level of the factor.

For novel levels, a slightly timmed average of the coefficients
is returned.

With partial pooling, a hierarchical generalized linear model
is fit using \code{\link[rstanarm:stan_glmer]{rstanarm::stan_glmer()}} and no intercept via\preformatted{  stan_glmer(outcome ~ (1 | predictor), data = data, ...)
}

where the \code{...} include the \code{family} argument (automatically
set by the step) as well as any arguments given to the \code{options}
argument to the step. Relevant options include \code{chains}, \code{iter},
\code{cores}, and arguments for the priors (see the links in the
References below). \code{prior_intercept} is the argument that has the
most effect on the amount of shrinkage.

For partial pooling when \code{stan = FALSE}, the embedding is estimated with\preformatted{  glmer(outcome ~ (1 | predictor), data = data, ...)
}
}
\examples{
library(recipes)
library(dplyr)

data(okc)

not_pooled <- recipe(Class ~ age + location, data = okc) \%>\%
  step_embed(location, outcome = vars(Class))

# See https://topepo.github.io/embed/ for examples
}
\references{
Zumel N and Mount J (2017) "vtreat: a data.frame Processor for
Predictive Modeling," arXiv:1611.09477

"Hierarchical Partial Pooling for Repeated Binary Trials"
\url{https://tinyurl.com/stan-pooling}

"Prior Distributions for `rstanarm`` Models"
\url{https://tinyurl.com/stan-priors}

"Estimating Generalized (Non-)Linear Models with Group-Specific
Terms with \code{rstanarm}" \url{https://tinyurl.com/stan-glm-grouped}
}
\concept{preprocessing encoding}
\keyword{datagen}
