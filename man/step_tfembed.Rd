% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tf.R
\name{step_tfembed}
\alias{step_tfembed}
\alias{tidy.step_tfembed}
\alias{tfembed_control}
\title{Encoding Factors into Multiple Columns}
\usage{
step_tfembed(recipe, ..., role = NA, trained = FALSE, outcome = NULL,
  number = 2, hidden = 0, options = tfembed_control(),
  mapping = NULL, history = NULL, skip = FALSE)

\method{tidy}{step_tfembed}(x, ...)

tfembed_control(loss = "mse", metrics = NULL, optimizer = "sgd",
  epochs = 20, validation_split = 0, batch_size = 32, verbose = 0)
}
\arguments{
\item{recipe}{A recipe object. The step will be added to the
sequence of operations for this recipe.}

\item{...}{One or more selector functions to choose variables.
For \code{step_tfembed}, this indicates the variables to be encoded
into a numeric format. See \code{\link[recipes:selections]{recipes::selections()}} for more details. For
the \code{tidy} method, these are not currently used.}

\item{role}{Not used by this step since no new variables are
created.}

\item{trained}{A logical to indicate if the quantities for
preprocessing have been estimated.}

\item{outcome}{A call to \code{vars} to specify which variable is
used as the outcome in the generalized linear model. Only
numeric and two-level factors are currently supported.}

\item{number}{An integer for the number of resulting variables.}

\item{hidden}{An integer for the number of hidden units in a dense ReLu
layer between the embedding and output later. Use a
value of zero for no intermediate layer (see Details below).}

\item{options}{A list of options for the model fitting process.}

\item{mapping}{A list of tibble results that define the
encoding. This is \code{NULL} until the step is trained by
\code{\link[recipes:prep.recipe]{recipes::prep.recipe()}}.}

\item{history}{A tibble with the convergence statistics for each term. This
is \code{NULL} until the step is trained by \code{\link[recipes:prep.recipe]{recipes::prep.recipe()}}.}

\item{skip}{A logical. Should the step be skipped when the
recipe is baked by \code{\link[recipes:bake.recipe]{recipes::bake.recipe()}}? While all operations are baked
when \code{\link[recipes:prep.recipe]{recipes::prep.recipe()}} is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using \code{skip = TRUE} as it may affect
the computations for subsequent operations}

\item{x}{A \code{step_tfembed} object.}

\item{optimizer, loss, metrics}{Arguments to pass to \code{\link[keras:compile]{keras::compile()}}}

\item{epochs, validation_split, batch_size, verbose}{Arguments to pass to \code{\link[keras:fit]{keras::fit()}}}
}
\value{
An updated version of \code{recipe} with the new step added
to the sequence of existing steps (if any). For the \code{tidy}
method, a tibble with columns \code{terms} (the selectors or
variables for encoding), \code{level} (the factor levels), and
several columns beginning with the prefix \code{emb} (the
encodings).
}
\description{
\code{step_tfembed} creates a \emph{specification} of a recipe step that
will convert a nominal (i.e. factor) predictor into a set of
scores derived from a tensorflow model via a word-embedding model.
\code{tfembed_control} is a simple wrapper for setting default options.
}
\details{
Factor levels are initially assigned at random to the
new variables and these variables are used in a neural network
to optimize both the allocation of levels to new columns as well
as estimating a model to predict the outcome. See Section 6.1.2
of Francois and Allaire (2018) for more details.

The new variables are mapped to the specific levels seen at the
time of model training and an extra instance of the variables
are used for new levels of the factor.

This model is trained separately for each factor predictor
given in the recipe step.

When the outcome is numeric, a linear activation function is
used in the last layer while softmax is used for factor outcomes
(with any number of levels).

For example, the \code{keras} code for a numeric outcome and no hidden units used
here would be\preformatted{  keras_model_sequential() \\\\%>\\\\% 
  layer_embedding(
    input_dim = num_factor_levels_x + 1,
    output_dim = number,
    input_length = 1
  ) \\\\%>\\\\%
  layer_flatten() \\\\%>\\\\%
  layer_dense(units = 1, activation = 'linear')
}

If a factor outcome is used and 10 hidden units were requested, the code
would be\preformatted{  keras_model_sequential() \\\\%>\\\\% 
  layer_embedding(
    input_dim = num_factor_levels_x + 1,
    output_dim = number,
    input_length = 1
   ) \\\\%>\\\\%
  layer_flatten() \\\\%>\\\\%
  layer_dense(units = 10, activation = "relu") \\\\%>\\\\%
  layer_dense(units = num_factor_levels_y, activation = 'softmax')
}

Also note that it may be difficult to obtain reproducible
results using this step due to the nature of Tensorflow (see
link in References).

tensorflow models cannot be run in parallel within the same
session (via \code{foreach}) or the \code{parallel} package. If using a
recipes with this step with \code{caret}, avoid parallel processing.
}
\examples{
data(okc)

rec <- recipe(Class ~ age + location, data = okc) \%>\%
  step_tfembed(location, outcome = vars(Class),
               options = tfembed_control(epochs = 10))

# See https://topepo.github.io/embed/ for examples
}
\references{
Francois C and Allaire JJ (2018) \emph{Deep Learning with R}, Manning

"How can I obtain reproducible results using Keras during development?"
\url{https://tinyurl.com/keras-repro}
}
\concept{preprocessing encoding}
\keyword{datagen}
